{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-0"
   },
   "source": [
    "# Catalan Lecture Processor (faster-whisper)\n",
    "\n",
    "Upload a university lecture recorded in Catalan and get:\n",
    "- **Full transcription** (using faster-whisper with Silero VAD)\n",
    "- **Cleaned text** (filler words removed, restructured into paragraphs)\n",
    "- **Translations** (Spanish, English, Portuguese, Italian)\n",
    "- **Summary** with key concepts\n",
    "- **PowerPoint slides**\n",
    "\n",
    "## Instructions\n",
    "1. **Runtime > Change runtime type > T4 GPU** (important!)\n",
    "2. **Click the play button below** (or press Shift+Enter)\n",
    "3. Wait ~2-3 minutes for setup and model downloads\n",
    "4. A **public link** will appear at the bottom \u2014 open it on your phone or computer\n",
    "5. Upload your audio file, select languages, click **Process Lecture**\n",
    "\n",
    "> **Optional:** To enable AI-powered summaries, paste a free Gemini API key below (get one at https://ai.google.dev/). Leave it empty to skip \u2014 everything else still works.\n",
    "\n",
    "## \u23f1 Expected processing times (T4 GPU)\n",
    "| Audio length | Transcription | Translation | Total |\n",
    "|---|---|---|---|\n",
    "| 10 min | ~1-2 min | ~1 min | ~3 min |\n",
    "| 30 min | ~3-5 min | ~2 min | ~8 min |\n",
    "| 60 min | ~5-10 min | ~5 min | ~18 min |\n",
    "\n",
    "> **Note:** This version uses **faster-whisper** with Voice Activity Detection (VAD), which is 2-4x faster than the original and avoids the repeated-word hallucination issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell-1"
   },
   "outputs": [],
   "source": [
    "#@title \u25b6\ufe0f Click the play button to start (or press Shift+Enter)\n",
    "GEMINI_API_KEY = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# SETUP \u2014 installing dependencies (this takes ~60 seconds)\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "import subprocess, sys, os\n",
    "\n",
    "# Reduce CUDA memory fragmentation on T4 GPUs\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "print(\"\\u2501\" * 60)\n",
    "print(\"  Setting up... this takes 1-2 minutes. Please wait.\")\n",
    "print(\"\\u2501\" * 60)\n",
    "\n",
    "print(\"\\n[1/3] Installing Python packages...\")\n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "    \"faster-whisper\", \"transformers\", \"gradio\", \"python-pptx\",\n",
    "    \"google-genai\", \"huggingface-hub\",\n",
    "    \"sentencepiece\", \"protobuf\", \"pydub\", \"tqdm\",\n",
    "], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "print(\"    \\u2713 Packages installed\")\n",
    "\n",
    "print(\"[2/3] Installing ffmpeg...\")\n",
    "subprocess.check_call(\n",
    "    [\"apt-get\", \"-qq\", \"install\", \"-y\", \"ffmpeg\"],\n",
    "    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL,\n",
    ")\n",
    "print(\"    \\u2713 ffmpeg installed\")\n",
    "\n",
    "print(\"[3/3] Checking GPU...\")\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"    \\u2713 GPU: {torch.cuda.get_device_name(0)} ({gpu_mem:.1f} GB)\")\n",
    "else:\n",
    "    print(\"    \\u2717 No GPU detected! Go to Runtime > Change runtime type > T4 GPU\")\n",
    "\n",
    "print(\"\\n\\u2501\" * 60)\n",
    "print(\"  Setup complete. Loading application...\")\n",
    "print(\"\\u2501\" * 60)\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# CONFIGURATION\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "import re\n",
    "import gc\n",
    "import time\n",
    "import logging\n",
    "import tempfile\n",
    "import shutil\n",
    "import zipfile\n",
    "from datetime import date\n",
    "\n",
    "import gradio as gr\n",
    "from pptx import Presentation as PptxPresentation\n",
    "from pptx.util import Inches, Pt\n",
    "from pptx.dml.color import RGBColor\n",
    "from pptx.enum.text import PP_ALIGN\n",
    "\n",
    "# Gemini API key\n",
    "if GEMINI_API_KEY:\n",
    "    os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
    "    print(\"\\u2713 Gemini API key set \\u2014 summaries enabled\")\n",
    "else:\n",
    "    print(\"\\u2139 No Gemini key \\u2014 summaries will be skipped (everything else works fine)\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\")\n",
    "logger = logging.getLogger(\"lecture_processor\")\n",
    "\n",
    "WHISPER_MODEL = \"large-v3\"\n",
    "NLLB_MODEL = \"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "LANGUAGE_CODES = {\n",
    "    \"Catalan\": \"cat_Latn\",\n",
    "    \"Spanish\": \"spa_Latn\",\n",
    "    \"English\": \"eng_Latn\",\n",
    "    \"Portuguese\": \"por_Latn\",\n",
    "    \"Italian\": \"ita_Latn\",\n",
    "}\n",
    "TARGET_LANGUAGES = [\"Spanish\", \"English\", \"Portuguese\", \"Italian\"]\n",
    "\n",
    "CATALAN_FILLERS = [\n",
    "    r\"per dir-ho d'alguna manera\", r\"diguem-ne\", r\"a veure\", r\"o sigui\",\n",
    "    r\"vull dir\", r\"\\u00e9s a dir\", r\"llavors\", r\"bueno\", r\"doncs\",\n",
    "    r\"saps\\?\", r\"vale\", r\"clar\", r\"oi\\?\", r\"no\\?\", r\"b\\u00e9\",\n",
    "    r\"ehm+\", r\"eh+\", r\"mm+\", r\"um+\", r\"ah+\",\n",
    "]\n",
    "\n",
    "CLEANUP_PROMPT = \"\"\"You are a text editor. The following is a transcription of a university \\\n",
    "lecture in Catalan. Clean it up by:\n",
    "1. Organizing into logical paragraphs\n",
    "2. Fixing any obvious transcription errors\n",
    "3. Removing remaining verbal fillers or repetitions\n",
    "4. Do NOT change the language or the meaning\n",
    "5. Do NOT add any commentary or explanations\n",
    "6. Return ONLY the cleaned text\n",
    "\n",
    "Transcription:\n",
    "{text}\"\"\"\n",
    "\n",
    "SUMMARY_PROMPT = \"\"\"You are an academic assistant. Summarize the following university lecture \\\n",
    "transcript. The lecture is in {language}. Provide your summary in {language}.\n",
    "\n",
    "Format your response as:\n",
    "## Main Topics\n",
    "- [bullet points of 5-10 key topics covered]\n",
    "\n",
    "## Detailed Summary\n",
    "[2-3 paragraphs summarizing the lecture content]\n",
    "\n",
    "## Key Terms\n",
    "- [list of important technical terms or concepts mentioned]\n",
    "\n",
    "Transcript:\n",
    "{text}\"\"\"\n",
    "\n",
    "CHUNK_SUMMARY_PROMPT = \"\"\"You are an academic assistant. Summarize this section of a university \\\n",
    "lecture transcript. The lecture is in {language}. Provide a concise summary in {language} \\\n",
    "capturing the key points, concepts, and any important terminology.\n",
    "\n",
    "Section:\n",
    "{text}\"\"\"\n",
    "\n",
    "SUPPORTED_AUDIO_FORMATS = [\".m4a\", \".mp3\", \".wav\", \".ogg\", \".webm\", \".flac\"]\n",
    "NLLB_MAX_LENGTH = 512\n",
    "NLLB_BATCH_MAX_TOKENS = 400\n",
    "LLM_CHUNK_MAX_WORDS = 3000\n",
    "SLIDE_MAX_BULLETS = 6\n",
    "TOTAL_STEPS = 5\n",
    "\n",
    "COLOR_TITLE = RGBColor(0x1A, 0x47, 0x8A)\n",
    "COLOR_BODY = RGBColor(0x33, 0x33, 0x33)\n",
    "COLOR_LIGHT = RGBColor(0x85, 0x92, 0x9E)\n",
    "\n",
    "\n",
    "def _free_gpu():\n",
    "    \"\"\"Release all GPU memory between pipeline steps.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# HELPER FUNCTIONS\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def _build_filler_pattern(fillers):\n",
    "    sorted_fillers = sorted(fillers, key=len, reverse=True)\n",
    "    pattern = r\"\\b(?:\" + \"|\".join(sorted_fillers) + r\")\\b\"\n",
    "    return re.compile(pattern, re.IGNORECASE | re.UNICODE)\n",
    "\n",
    "\n",
    "def _chunk_text(text, max_words=LLM_CHUNK_MAX_WORDS):\n",
    "    sentences = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "    chunks, current_chunk, current_count = [], [], 0\n",
    "    for sentence in sentences:\n",
    "        word_count = len(sentence.split())\n",
    "        if current_count + word_count > max_words and current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk, current_count = [], 0\n",
    "        current_chunk.append(sentence)\n",
    "        current_count += word_count\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def _progress_bar(fraction, width=30):\n",
    "    fraction = max(0.0, min(1.0, fraction))\n",
    "    filled = int(width * fraction)\n",
    "    bar = \"\\u2588\" * filled + \"\\u2591\" * (width - filled)\n",
    "    return f\"[{bar}] {fraction * 100:.0f}%\"\n",
    "\n",
    "\n",
    "def _step(n, title, detail=\"\", progress=None):\n",
    "    header = f\"Step {n}/{TOTAL_STEPS} \\u2014 {title}\"\n",
    "    if progress is not None:\n",
    "        header += f\"  {_progress_bar(progress)}\"\n",
    "    if detail:\n",
    "        return f\"{header}\\n{detail}\"\n",
    "    return header\n",
    "\n",
    "\n",
    "def _remove_hallucinations(text):\n",
    "    \"\"\"Remove repeated word/phrase sequences caused by Whisper hallucinations.\"\"\"\n",
    "    # Single word repeated 4+ times: \\\"dem\\u00e0 dem\\u00e0 dem\\u00e0 dem\\u00e0\\\" \\u2192 \\\"dem\\u00e0\\\"\n",
    "    cleaned = re.sub(r'\\b(\\w+)([ \\t]+\\1){3,}\\b', r'\\1', text)\n",
    "    # Short phrase (2-10 words) repeated 3+ times\n",
    "    cleaned = re.sub(r'((?:\\b\\w+\\b[ \\t]*){2,10}?)\\1{2,}', r'\\1', cleaned)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# TRANSCRIBER (faster-whisper + Silero VAD)\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "class Transcriber:\n",
    "    def __init__(self):\n",
    "        self._model = None\n",
    "\n",
    "    def _load_model(self):\n",
    "        from faster_whisper import WhisperModel\n",
    "\n",
    "        logger.info(\"Loading faster-whisper model: %s\", WHISPER_MODEL)\n",
    "        self._model = WhisperModel(\n",
    "            WHISPER_MODEL,\n",
    "            device=\"cuda\",\n",
    "            compute_type=\"float16\",\n",
    "        )\n",
    "        logger.info(\"faster-whisper model loaded on GPU\")\n",
    "\n",
    "    def transcribe(self, audio_path, progress_callback=None):\n",
    "        if self._model is None:\n",
    "            if progress_callback:\n",
    "                progress_callback(0.05, desc=\"Loading Whisper model...\")\n",
    "            self._load_model()\n",
    "\n",
    "        if progress_callback:\n",
    "            progress_callback(0.10, desc=\"Transcribing audio...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        segments_gen, info = self._model.transcribe(\n",
    "            audio_path,\n",
    "            language=\"ca\",\n",
    "            task=\"transcribe\",\n",
    "            beam_size=5,\n",
    "            vad_filter=True,\n",
    "            vad_parameters=dict(\n",
    "                min_silence_duration_ms=500,\n",
    "                speech_pad_ms=400,\n",
    "            ),\n",
    "            condition_on_previous_text=False,\n",
    "        )\n",
    "\n",
    "        segments = []\n",
    "        text_parts = []\n",
    "        for seg in segments_gen:\n",
    "            seg_text = seg.text.strip()\n",
    "            if seg_text:\n",
    "                text_parts.append(seg_text)\n",
    "                segments.append({\n",
    "                    \"start\": seg.start,\n",
    "                    \"end\": seg.end,\n",
    "                    \"text\": seg_text,\n",
    "                })\n",
    "\n",
    "        full_text = \" \".join(text_parts)\n",
    "\n",
    "        # Safety net: remove any remaining hallucinated repetitions\n",
    "        full_text = _remove_hallucinations(full_text)\n",
    "\n",
    "        if progress_callback:\n",
    "            progress_callback(0.50, desc=\"Transcription complete\")\n",
    "\n",
    "        return {\n",
    "            \"text\": full_text,\n",
    "            \"segments\": segments,\n",
    "            \"language\": info.language,\n",
    "            \"duration_seconds\": time.time() - t0,\n",
    "        }\n",
    "\n",
    "    def unload(self):\n",
    "        \"\"\"Free GPU memory by removing the model.\"\"\"\n",
    "        if self._model is not None:\n",
    "            del self._model\n",
    "            self._model = None\n",
    "            _free_gpu()\n",
    "            logger.info(\"Whisper model unloaded from GPU\")\n",
    "\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# TEXT CLEANER\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        self._filler_pattern = _build_filler_pattern(CATALAN_FILLERS)\n",
    "\n",
    "    def _has_gemini(self):\n",
    "        return bool(os.environ.get(\"GEMINI_API_KEY\"))\n",
    "\n",
    "    def _call_gemini(self, prompt):\n",
    "        from google import genai\n",
    "        client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\", contents=prompt\n",
    "        )\n",
    "        return response.text.strip()\n",
    "\n",
    "    def regex_clean(self, text):\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        cleaned = self._filler_pattern.sub(\"\", text)\n",
    "        cleaned = re.sub(r\"[ \\t]+\", \" \", cleaned)\n",
    "        cleaned = re.sub(r\" ([.,;:!?])\", r\"\\1\", cleaned)\n",
    "        cleaned = re.sub(r\"\\n{3,}\", \"\\n\\n\", cleaned)\n",
    "        cleaned = cleaned.strip()\n",
    "        cleaned = re.sub(\n",
    "            r\"([.!?]\\s+)([a-z\\u00e1\\u00e0\\u00e9\\u00e8\\u00ed\\u00ef\\u00f3\\u00f2\\u00fa\\u00fc\\u00e7])\",\n",
    "            lambda m: m.group(1) + m.group(2).upper(), cleaned\n",
    "        )\n",
    "        if cleaned and cleaned[0].islower():\n",
    "            cleaned = cleaned[0].upper() + cleaned[1:]\n",
    "        return cleaned\n",
    "\n",
    "    def clean(self, text, progress_callback=None):\n",
    "        if progress_callback:\n",
    "            progress_callback(0.50, desc=\"Removing filler words...\")\n",
    "        regex_cleaned = self.regex_clean(text)\n",
    "\n",
    "        llm_cleaned = None\n",
    "        if self._has_gemini():\n",
    "            if progress_callback:\n",
    "                progress_callback(0.52, desc=\"Restructuring text with Gemini...\")\n",
    "            try:\n",
    "                chunks = _chunk_text(regex_cleaned)\n",
    "                parts = []\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    if progress_callback:\n",
    "                        frac = 0.52 + 0.08 * ((i + 1) / len(chunks))\n",
    "                        progress_callback(frac, desc=f\"Restructuring chunk {i+1}/{len(chunks)}...\")\n",
    "                    prompt = CLEANUP_PROMPT.format(text=chunk)\n",
    "                    parts.append(self._call_gemini(prompt))\n",
    "                llm_cleaned = \"\\n\\n\".join(parts)\n",
    "            except Exception as e:\n",
    "                logger.warning(\"Gemini cleanup failed: %s\", e)\n",
    "\n",
    "        return {\n",
    "            \"regex_cleaned\": regex_cleaned,\n",
    "            \"llm_cleaned\": llm_cleaned,\n",
    "            \"best\": llm_cleaned if llm_cleaned else regex_cleaned,\n",
    "        }\n",
    "\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# TRANSLATOR\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "class Translator:\n",
    "    def __init__(self):\n",
    "        self._model = None\n",
    "        self._tokenizer = None\n",
    "\n",
    "    def _load_model(self):\n",
    "        from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "        import torch\n",
    "\n",
    "        logger.info(\"Loading NLLB-200 translation model...\")\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(NLLB_MODEL)\n",
    "        self._model = AutoModelForSeq2SeqLM.from_pretrained(NLLB_MODEL)\n",
    "        if torch.cuda.is_available():\n",
    "            self._model = self._model.half().to(\"cuda\")\n",
    "        logger.info(\"Translation model loaded\")\n",
    "\n",
    "    def _ensure_loaded(self):\n",
    "        if self._model is None:\n",
    "            self._load_model()\n",
    "\n",
    "    def _split_into_batches(self, text):\n",
    "        sentences = re.split(r\"(?<=[.!?;:])\\s+\", text)\n",
    "        batches, current_batch, current_tokens = [], [], 0\n",
    "        for sentence in sentences:\n",
    "            tokens = len(self._tokenizer.encode(sentence, add_special_tokens=False))\n",
    "            if current_tokens + tokens > NLLB_BATCH_MAX_TOKENS and current_batch:\n",
    "                batches.append(\" \".join(current_batch))\n",
    "                current_batch, current_tokens = [], 0\n",
    "            current_batch.append(sentence)\n",
    "            current_tokens += tokens\n",
    "        if current_batch:\n",
    "            batches.append(\" \".join(current_batch))\n",
    "        return batches\n",
    "\n",
    "    def translate_text(self, text, source_lang, target_lang):\n",
    "        if not text or not text.strip():\n",
    "            return \"\"\n",
    "        self._ensure_loaded()\n",
    "\n",
    "        src_code = LANGUAGE_CODES[source_lang]\n",
    "        tgt_code = LANGUAGE_CODES[target_lang]\n",
    "        tgt_token_id = self._tokenizer.convert_tokens_to_ids(tgt_code)\n",
    "\n",
    "        self._tokenizer.src_lang = src_code\n",
    "        batches = self._split_into_batches(text)\n",
    "        translated_parts = []\n",
    "\n",
    "        device = \"cuda\" if next(self._model.parameters()).is_cuda else \"cpu\"\n",
    "        for batch in batches:\n",
    "            inputs = self._tokenizer(\n",
    "                batch, return_tensors=\"pt\", padding=True,\n",
    "                truncation=True, max_length=NLLB_MAX_LENGTH\n",
    "            )\n",
    "            if device == \"cuda\":\n",
    "                inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "            outputs = self._model.generate(\n",
    "                **inputs, forced_bos_token_id=tgt_token_id, max_length=NLLB_MAX_LENGTH\n",
    "            )\n",
    "            decoded = self._tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            translated_parts.extend(decoded)\n",
    "\n",
    "        return \" \".join(translated_parts)\n",
    "\n",
    "    def unload(self):\n",
    "        \"\"\"Free GPU memory by removing the model.\"\"\"\n",
    "        if self._model is not None:\n",
    "            del self._model\n",
    "            del self._tokenizer\n",
    "            self._model = None\n",
    "            self._tokenizer = None\n",
    "            _free_gpu()\n",
    "            logger.info(\"Translation model unloaded from GPU\")\n",
    "\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# SUMMARIZER\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "class Summarizer:\n",
    "    def _has_gemini(self):\n",
    "        return bool(os.environ.get(\"GEMINI_API_KEY\"))\n",
    "\n",
    "    def _call_gemini(self, prompt):\n",
    "        from google import genai\n",
    "        client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\", contents=prompt\n",
    "        )\n",
    "        return response.text.strip()\n",
    "\n",
    "    def summarize(self, text, language=\"Catalan\", progress_callback=None):\n",
    "        if progress_callback:\n",
    "            progress_callback(0.80, desc=\"Generating summary...\")\n",
    "\n",
    "        if not self._has_gemini():\n",
    "            logger.info(\"No Gemini key, skipping summarization\")\n",
    "            return {\"raw_summary\": None, \"main_topics\": [], \"detailed_summary\": \"\",\n",
    "                    \"key_terms\": [], \"sections\": []}\n",
    "\n",
    "        word_count = len(text.split())\n",
    "        try:\n",
    "            if word_count > LLM_CHUNK_MAX_WORDS:\n",
    "                raw = self._map_reduce(text, language, progress_callback)\n",
    "            else:\n",
    "                raw = self._call_gemini(SUMMARY_PROMPT.format(language=language, text=text))\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Summarization failed: %s\", e)\n",
    "            return {\"raw_summary\": None, \"main_topics\": [], \"detailed_summary\": \"\",\n",
    "                    \"key_terms\": [], \"sections\": []}\n",
    "\n",
    "        if progress_callback:\n",
    "            progress_callback(0.90, desc=\"Summary complete\")\n",
    "\n",
    "        return {\n",
    "            \"raw_summary\": raw,\n",
    "            \"main_topics\": self._extract_list(raw, \"Main Topics\"),\n",
    "            \"detailed_summary\": self._extract_section(raw, \"Detailed Summary\"),\n",
    "            \"key_terms\": self._extract_list(raw, \"Key Terms\"),\n",
    "            \"sections\": self._parse_sections(raw),\n",
    "        }\n",
    "\n",
    "    def _map_reduce(self, text, language, progress_callback=None):\n",
    "        chunks = _chunk_text(text)\n",
    "        summaries = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if progress_callback:\n",
    "                frac = 0.82 + 0.06 * ((i + 1) / len(chunks))\n",
    "                progress_callback(frac, desc=f\"Summarizing section {i+1}/{len(chunks)}...\")\n",
    "            prompt = CHUNK_SUMMARY_PROMPT.format(language=language, text=chunk)\n",
    "            summaries.append(self._call_gemini(prompt))\n",
    "        combined = \"\\n\\n\".join(summaries)\n",
    "        if progress_callback:\n",
    "            progress_callback(0.88, desc=\"Generating final summary...\")\n",
    "        return self._call_gemini(SUMMARY_PROMPT.format(language=language, text=combined))\n",
    "\n",
    "    def _parse_sections(self, text):\n",
    "        sections, title, bullets = [], None, []\n",
    "        for line in text.split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"## \"):\n",
    "                if title:\n",
    "                    sections.append({\"title\": title, \"bullets\": bullets})\n",
    "                title, bullets = line[3:].strip(), []\n",
    "            elif (line.startswith(\"- \") or line.startswith(\"* \")) and title:\n",
    "                bullets.append(line[2:].strip())\n",
    "            elif line and title:\n",
    "                bullets.append(line)\n",
    "        if title:\n",
    "            sections.append({\"title\": title, \"bullets\": bullets})\n",
    "        return sections\n",
    "\n",
    "    def _extract_list(self, text, header):\n",
    "        in_section, items = False, []\n",
    "        for line in text.split(\"\\n\"):\n",
    "            s = line.strip()\n",
    "            if s.startswith(\"## \") and header.lower() in s.lower():\n",
    "                in_section = True\n",
    "            elif s.startswith(\"## \") and in_section:\n",
    "                break\n",
    "            elif in_section and (s.startswith(\"- \") or s.startswith(\"* \")):\n",
    "                items.append(s[2:].strip())\n",
    "        return items\n",
    "\n",
    "    def _extract_section(self, text, header):\n",
    "        in_section, parts = False, []\n",
    "        for line in text.split(\"\\n\"):\n",
    "            s = line.strip()\n",
    "            if s.startswith(\"## \") and header.lower() in s.lower():\n",
    "                in_section = True\n",
    "            elif s.startswith(\"## \") and in_section:\n",
    "                break\n",
    "            elif in_section and s:\n",
    "                parts.append(s)\n",
    "        return \"\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# SLIDE GENERATOR\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "class SlideGenerator:\n",
    "    def generate(self, summary_data, title=\"Lecture Summary\", output_path=None):\n",
    "        prs = PptxPresentation()\n",
    "        prs.slide_width = Inches(13.333)\n",
    "        prs.slide_height = Inches(7.5)\n",
    "\n",
    "        slide = prs.slides.add_slide(prs.slide_layouts[0])\n",
    "        slide.shapes.title.text = title\n",
    "        for p in slide.shapes.title.text_frame.paragraphs:\n",
    "            p.font.size, p.font.color.rgb, p.font.bold = Pt(36), COLOR_TITLE, True\n",
    "        if len(slide.placeholders) > 1:\n",
    "            slide.placeholders[1].text = f\"Generated {date.today().isoformat()}\"\n",
    "\n",
    "        topics = summary_data.get(\"main_topics\", [])\n",
    "        if topics:\n",
    "            self._add_content_slide(prs, \"Overview\", topics)\n",
    "\n",
    "        for sec in summary_data.get(\"sections\", []):\n",
    "            if sec.get(\"title\") and sec.get(\"bullets\"):\n",
    "                self._add_content_slide(prs, sec[\"title\"], sec[\"bullets\"])\n",
    "\n",
    "        terms = summary_data.get(\"key_terms\", [])\n",
    "        if terms:\n",
    "            self._add_content_slide(prs, \"Key Terms & Concepts\", terms)\n",
    "\n",
    "        slide = prs.slides.add_slide(prs.slide_layouts[0])\n",
    "        slide.shapes.title.text = \"End of Summary\"\n",
    "        for p in slide.shapes.title.text_frame.paragraphs:\n",
    "            p.font.size, p.font.color.rgb = Pt(36), COLOR_TITLE\n",
    "            p.alignment = PP_ALIGN.CENTER\n",
    "\n",
    "        if output_path is None:\n",
    "            output_path = f\"/tmp/lecture_slides_{date.today().isoformat()}.pptx\"\n",
    "        prs.save(output_path)\n",
    "        return output_path\n",
    "\n",
    "    def _add_content_slide(self, prs, title_text, bullets):\n",
    "        for idx in range(0, len(bullets), SLIDE_MAX_BULLETS):\n",
    "            page = bullets[idx:idx + SLIDE_MAX_BULLETS]\n",
    "            st = f\"{title_text} (cont.)\" if idx > 0 else title_text\n",
    "            slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "            slide.shapes.title.text = st\n",
    "            for p in slide.shapes.title.text_frame.paragraphs:\n",
    "                p.font.size, p.font.color.rgb, p.font.bold = Pt(28), COLOR_TITLE, True\n",
    "            if len(slide.placeholders) > 1:\n",
    "                tf = slide.placeholders[1].text_frame\n",
    "                tf.clear()\n",
    "                for i, b in enumerate(page):\n",
    "                    p = tf.paragraphs[0] if i == 0 else tf.add_paragraph()\n",
    "                    p.text = b\n",
    "                    p.font.size, p.font.color.rgb = Pt(18), COLOR_BODY\n",
    "                    p.space_after = Pt(8)\n",
    "\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# PIPELINE ORCHESTRATOR\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "class LectureProcessor:\n",
    "    def __init__(self):\n",
    "        self._transcriber = Transcriber()\n",
    "        self._cleaner = TextCleaner()\n",
    "        self._translator = Translator()\n",
    "        self._summarizer = Summarizer()\n",
    "        self._slides = SlideGenerator()\n",
    "\n",
    "    def process(self, audio_path, target_languages):\n",
    "        results = {\n",
    "            \"transcript_raw\": None, \"transcript_clean\": None,\n",
    "            \"translations\": {}, \"summary\": None,\n",
    "            \"summaries\": {}, \"summaries_data\": {},\n",
    "            \"all_files\": {}, \"errors\": [], \"timings\": {},\n",
    "        }\n",
    "        output_dir = tempfile.mkdtemp(prefix=\"lecture_\")\n",
    "\n",
    "        audio_copy = os.path.join(output_dir, os.path.basename(audio_path))\n",
    "        shutil.copy2(audio_path, audio_copy)\n",
    "        audio_path = audio_copy\n",
    "\n",
    "        ext = os.path.splitext(audio_path)[1].lower()\n",
    "        if ext not in SUPPORTED_AUDIO_FORMATS:\n",
    "            results[\"errors\"].append(f\"Unsupported format: {ext}\")\n",
    "            yield (f\"Error: Unsupported format {ext}\", results)\n",
    "            return\n",
    "\n",
    "        # Step 1: Transcribe\n",
    "        yield (_step(1, \"Transcribing\",\n",
    "            \"Using faster-whisper with Voice Activity Detection.\\n\"\n",
    "            \"A 1-hour lecture typically takes ~5-10 minutes on T4 GPU.\"), results)\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            tr = self._transcriber.transcribe(audio_path)\n",
    "            results[\"transcript_raw\"] = tr[\"text\"]\n",
    "            results[\"timings\"][\"transcription\"] = time.time() - t0\n",
    "            path = os.path.join(output_dir, \"transcript_raw.txt\")\n",
    "            open(path, \"w\", encoding=\"utf-8\").write(results[\"transcript_raw\"])\n",
    "            results[\"all_files\"][\"transcript_raw.txt\"] = path\n",
    "        except Exception as e:\n",
    "            results[\"errors\"].append(f\"Transcription failed: {e}\")\n",
    "            yield (f\"Transcription failed: {e}\", results)\n",
    "            return\n",
    "        finally:\n",
    "            # Always unload Whisper to free GPU for translation\n",
    "            self._transcriber.unload()\n",
    "\n",
    "        # Step 2: Clean text\n",
    "        yield (_step(2, \"Cleaning text\", \"Removing filler words...\"), results)\n",
    "        t0 = time.time()\n",
    "        try:\n",
    "            cr = self._cleaner.clean(results[\"transcript_raw\"])\n",
    "            results[\"transcript_clean\"] = cr[\"best\"]\n",
    "            results[\"timings\"][\"cleanup\"] = time.time() - t0\n",
    "            path = os.path.join(output_dir, \"transcript_clean.txt\")\n",
    "            open(path, \"w\", encoding=\"utf-8\").write(results[\"transcript_clean\"])\n",
    "            results[\"all_files\"][\"transcript_clean.txt\"] = path\n",
    "        except Exception as e:\n",
    "            results[\"errors\"].append(f\"Cleanup failed: {e}\")\n",
    "            results[\"transcript_clean\"] = results[\"transcript_raw\"]\n",
    "\n",
    "        # Step 3: Translate\n",
    "        yield (_step(3, \"Translating\", \"Loading translation model...\"), results)\n",
    "        t0 = time.time()\n",
    "        text_to_translate = results[\"transcript_clean\"] or results[\"transcript_raw\"]\n",
    "        try:\n",
    "            self._translator._ensure_loaded()\n",
    "            num_langs = len(target_languages)\n",
    "            for i, lang in enumerate(target_languages):\n",
    "                frac = i / num_langs\n",
    "                yield (_step(3, \"Translating\", f\"{lang}...\", progress=frac), results)\n",
    "                try:\n",
    "                    results[\"translations\"][lang] = self._translator.translate_text(\n",
    "                        text_to_translate, \"Catalan\", lang\n",
    "                    )\n",
    "                    txt = results[\"translations\"][lang]\n",
    "                    if not txt.startswith(\"[Translation\"):\n",
    "                        fn = f\"translation_{lang.lower()}.txt\"\n",
    "                        path = os.path.join(output_dir, fn)\n",
    "                        open(path, \"w\", encoding=\"utf-8\").write(txt)\n",
    "                        results[\"all_files\"][fn] = path\n",
    "                except Exception as e:\n",
    "                    logger.error(\"Translation to %s failed: %s\", lang, e)\n",
    "                    results[\"translations\"][lang] = f\"[Translation to {lang} failed: {e}]\"\n",
    "            results[\"timings\"][\"translation\"] = time.time() - t0\n",
    "        except Exception as e:\n",
    "            results[\"errors\"].append(f\"Translation failed: {e}\")\n",
    "        finally:\n",
    "            # Unload translation model to free GPU for next run\n",
    "            self._translator.unload()\n",
    "\n",
    "        # Step 4: Summarize\n",
    "        t0 = time.time()\n",
    "        num_summary_langs = len(target_languages)\n",
    "        for lang_idx, lang in enumerate(target_languages):\n",
    "            translated_text = results[\"translations\"].get(lang, \"\")\n",
    "            if not translated_text or translated_text.startswith(\"[Translation\"):\n",
    "                continue\n",
    "            frac = lang_idx / num_summary_langs\n",
    "            yield (_step(4, \"Summarizing\", f\"{lang}...\", progress=frac), results)\n",
    "            try:\n",
    "                sd = self._summarizer.summarize(translated_text, lang)\n",
    "                raw = sd.get(\"raw_summary\")\n",
    "                results[\"summaries\"][lang] = raw\n",
    "                results[\"summaries_data\"][lang] = sd\n",
    "                if raw:\n",
    "                    fn = f\"summary_{lang.lower()}.md\"\n",
    "                    path = os.path.join(output_dir, fn)\n",
    "                    open(path, \"w\", encoding=\"utf-8\").write(raw)\n",
    "                    results[\"all_files\"][fn] = path\n",
    "            except Exception as e:\n",
    "                logger.error(\"Summarization for %s failed: %s\", lang, e)\n",
    "                results[\"errors\"].append(f\"Summarization ({lang}) failed: {e}\")\n",
    "        results[\"timings\"][\"summarization\"] = time.time() - t0\n",
    "\n",
    "        summary_parts = []\n",
    "        for lang in target_languages:\n",
    "            raw = results[\"summaries\"].get(lang)\n",
    "            if raw:\n",
    "                summary_parts.append(f\"## {lang}\\n\\n{raw}\")\n",
    "        results[\"summary\"] = \"\\n\\n---\\n\\n\".join(summary_parts) if summary_parts else None\n",
    "\n",
    "        # Step 5: Generate slides\n",
    "        yield (_step(5, \"Creating slides\"), results)\n",
    "        t0 = time.time()\n",
    "        slides_created = 0\n",
    "        audio_name = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "        for lang in target_languages:\n",
    "            sd = results.get(\"summaries_data\", {}).get(lang, {})\n",
    "            if not (sd.get(\"main_topics\") or sd.get(\"sections\")):\n",
    "                continue\n",
    "            try:\n",
    "                slides_path = os.path.join(output_dir, f\"lecture_slides_{lang.lower()}.pptx\")\n",
    "                self._slides.generate(sd, title=f\"{audio_name} ({lang})\", output_path=slides_path)\n",
    "                results[\"all_files\"][f\"lecture_slides_{lang.lower()}.pptx\"] = slides_path\n",
    "                slides_created += 1\n",
    "            except Exception as e:\n",
    "                logger.error(\"Slide generation for %s failed: %s\", lang, e)\n",
    "                results[\"errors\"].append(f\"Slides ({lang}) failed: {e}\")\n",
    "        if slides_created == 0:\n",
    "            results[\"errors\"].append(\"Slides skipped: no summary data available\")\n",
    "        results[\"timings\"][\"slides\"] = time.time() - t0\n",
    "\n",
    "        if results[\"all_files\"]:\n",
    "            try:\n",
    "                zip_path = os.path.join(output_dir, \"lecture_all_files.zip\")\n",
    "                with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "                    for filename, filepath in results[\"all_files\"].items():\n",
    "                        zf.write(filepath, filename)\n",
    "                results[\"zip_path\"] = zip_path\n",
    "            except Exception as e:\n",
    "                logger.error(\"ZIP creation failed: %s\", e)\n",
    "\n",
    "        timing_parts = [f\"{k}: {v:.1f}s\" for k, v in results[\"timings\"].items()]\n",
    "        status = \"Done! \" + \" | \".join(timing_parts) if timing_parts else \"Done!\"\n",
    "        yield (status, results)\n",
    "\n",
    "\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# GRADIO UI & LAUNCH\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "processor = LectureProcessor()\n",
    "\n",
    "with gr.Blocks(\n",
    "    title=\"Catalan Lecture Processor\",\n",
    "    theme=gr.themes.Soft(),\n",
    "    css=\".gradio-container { max-width: 960px !important; margin: auto; }\",\n",
    ") as app:\n",
    "\n",
    "    gr.Markdown(\n",
    "        \"# Catalan Lecture Processor\\n\"\n",
    "        \"Upload a lecture recording in Catalan. Get transcription, \"\n",
    "        \"translation, summary, and PowerPoint slides.\"\n",
    "    )\n",
    "\n",
    "    with gr.Group():\n",
    "        gr.Markdown(\"### Upload Audio\")\n",
    "        audio_input = gr.Audio(\n",
    "            label=\"Lecture audio (m4a, mp3, wav, ogg, webm, flac)\",\n",
    "            type=\"filepath\", sources=[\"upload\"],\n",
    "        )\n",
    "        target_langs = gr.CheckboxGroup(\n",
    "            choices=TARGET_LANGUAGES, value=[\"Spanish\", \"English\"],\n",
    "            label=\"Translate to:\",\n",
    "        )\n",
    "        process_btn = gr.Button(\"Process Lecture\", variant=\"primary\", size=\"lg\")\n",
    "\n",
    "    status_text = gr.Textbox(label=\"Status\", interactive=False, lines=3)\n",
    "    errors_text = gr.Textbox(label=\"Warnings\", interactive=False, lines=2, visible=False)\n",
    "\n",
    "    with gr.Tabs():\n",
    "        with gr.Tab(\"Transcript\"):\n",
    "            transcript_raw = gr.Textbox(label=\"Raw Transcription (Catalan)\", lines=12, max_lines=30)\n",
    "            transcript_clean = gr.Textbox(label=\"Cleaned Transcription\", lines=12, max_lines=30)\n",
    "\n",
    "        with gr.Tab(\"Translations\"):\n",
    "            trans_boxes = {}\n",
    "            for lang in TARGET_LANGUAGES:\n",
    "                trans_boxes[lang] = gr.Textbox(\n",
    "                    label=f\"{lang} Translation\", lines=10,\n",
    "                    visible=(lang in [\"Spanish\", \"English\"]),\n",
    "                )\n",
    "\n",
    "        with gr.Tab(\"Summary\"):\n",
    "            summary_output = gr.Markdown(label=\"Lecture Summary\")\n",
    "\n",
    "        with gr.Tab(\"Downloads\"):\n",
    "            download_zip = gr.File(\n",
    "                label=\"Download All (ZIP)\",\n",
    "                file_count=\"single\",\n",
    "            )\n",
    "            gr.Markdown(\"Or download individual files:\")\n",
    "            download_files = gr.File(label=\"Individual Files\", file_count=\"multiple\")\n",
    "\n",
    "    def update_visibility(languages):\n",
    "        return [gr.update(visible=(l in languages)) for l in TARGET_LANGUAGES]\n",
    "\n",
    "    target_langs.change(\n",
    "        update_visibility, inputs=[target_langs],\n",
    "        outputs=list(trans_boxes.values()),\n",
    "    )\n",
    "\n",
    "    def process_lecture(audio, languages):\n",
    "        if audio is None:\n",
    "            raise gr.Error(\"Please upload an audio file first.\")\n",
    "        if not languages:\n",
    "            raise gr.Error(\"Please select at least one target language.\")\n",
    "\n",
    "        def build_output(status, results):\n",
    "            raw = results.get(\"transcript_raw\") or \"\"\n",
    "            clean = results.get(\"transcript_clean\") or \"\"\n",
    "            translations = results.get(\"translations\", {})\n",
    "            trans_outputs = [translations.get(l, \"\") for l in TARGET_LANGUAGES]\n",
    "            summary = results.get(\"summary\") or \"\"\n",
    "            zip_path = results.get(\"zip_path\")\n",
    "            files_list = list(results.get(\"all_files\", {}).values()) or None\n",
    "            errors = results.get(\"errors\", [])\n",
    "            errors_str = \"\\n\".join(errors) if errors else \"\"\n",
    "            return (\n",
    "                status,\n",
    "                gr.update(value=errors_str, visible=bool(errors)),\n",
    "                raw, clean,\n",
    "                *trans_outputs,\n",
    "                summary, zip_path, files_list,\n",
    "            )\n",
    "\n",
    "        for status, results in processor.process(audio, languages):\n",
    "            yield build_output(status, results)\n",
    "\n",
    "    all_outputs = [\n",
    "        status_text, errors_text,\n",
    "        transcript_raw, transcript_clean,\n",
    "        *list(trans_boxes.values()),\n",
    "        summary_output, download_zip, download_files,\n",
    "    ]\n",
    "\n",
    "    process_btn.click(\n",
    "        process_lecture,\n",
    "        inputs=[audio_input, target_langs],\n",
    "        outputs=all_outputs,\n",
    "    )\n",
    "\n",
    "print(\"\\n\\u2501\" * 60)\n",
    "print(\"  \\u2713 Ready! The public link will appear below.\")\n",
    "print(\"\\u2501\" * 60 + \"\\n\")\n",
    "\n",
    "app.launch(\n",
    "    share=True,\n",
    "    debug=True,\n",
    ")"
   ]
  }
 ]
}
